{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For daily income maximization and income per work hours maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from holidays.countries.united_states import US\n",
    "# import holidays\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col, lit, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.context import SparkContext\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy import distance\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from functools import reduce\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence, permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ssl\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "\n",
    "spark = SparkSession.builder.appName(\"tripDataCSVLoad\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_fm =spark.read.parquet(\"spark-warehouse/trip_driver_fmax\")\n",
    "print(df_trip_fm.count())\n",
    "print(df_trip_fm.schema.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check suspicious taxi rotation activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### number of taxis driven per day\n",
    "\n",
    "df_taxi_driven_per_day_check = df_trip_fm.groupBy('driver_id','pickup_date','taxi_id').count()\\\n",
    "    .groupBy('driver_id','pickup_date').count().sort(F.desc('count')).toPandas()\n",
    "\n",
    "# df_taxi_driven_per_day_check.groupby(by='count',as_index=False)['driver_id'].count().sort_values(by='count',ascending=0)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Remove suspicious data of driver_id - pickup_date combo occupying more than 2 taxis in a day\n",
    "taxi_per_day_limit = 2\n",
    "df_combo_to_drop = df_taxi_driven_per_day_check.iloc[np.where(df_taxi_driven_per_day_check['count']>taxi_per_day_limit)].drop(columns='count',axis=1)\n",
    "\n",
    "#taxis occupied by driver in a day\tdriver-days\n",
    "# 18\t1\n",
    "# 17\t1\n",
    "# 16\t1\n",
    "# 15\t2\n",
    "# 13\t2\n",
    "# 12\t2\n",
    "# 11\t2\n",
    "# 10\t1\n",
    "# 9\t    5\n",
    "# 8\t    7\n",
    "# 7\t    11\n",
    "# 6\t    2\n",
    "# 5\t    17\n",
    "# 4\t    22\n",
    "# 3\t    199\n",
    "# 2\t    26248\n",
    "# 1\t    688944\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove suspicious Driver ID - Pickup Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_trip_fm.count())\n",
    "df_combo_to_drop_spark =spark.createDataFrame(df_combo_to_drop) \n",
    "\n",
    "joining_conds = [df_trip_fm.driver_id == df_combo_to_drop_spark.driver_id,\n",
    "                 df_trip_fm.pickup_date == df_combo_to_drop_spark.pickup_date]\n",
    "df_trip_fm = df_trip_fm.join(df_combo_to_drop_spark, joining_conds, 'leftanti')\n",
    "print(df_trip_fm.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate to arrive at taxi_driver_id - pickup_date level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check pickup_area distribution\n",
    "\n",
    "total_trips = df_trip_fm.count()\n",
    "df_pickup_area = df_trip_fm.groupBy('pickup_area').count().toPandas().sort_values(by='count',ascending=0)\n",
    "df_pickup_area['percentage'] = df_pickup_area['count'].apply(lambda x: round(float(x)/total_trips,2))\n",
    "df_pickup_area_filtered = df_pickup_area.iloc[np.where(df_pickup_area['percentage']>=0.01)]\n",
    "df_pickup_area_filtered['abbrv_pickup_area'] = df_pickup_area_filtered['pickup_area'].apply(lambda x: re.split(', ',x)[0])\n",
    "\n",
    "pickup_area_dict = df_pickup_area_filtered[['pickup_area','abbrv_pickup_area']].set_index('pickup_area').to_dict()['abbrv_pickup_area']\n",
    "\n",
    "pickup_area_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add flag weekend, driver_revenue, pickup_cat (pickup_period - pickup_area)\n",
    "getabbrv = udf(lambda x: 'others' if x not in pickup_area_dict.keys() else pickup_area_dict[x],StringType())\n",
    "\n",
    "flgweekday = udf(lambda x: 0 if x in [0,6] else 1,IntegerType())\n",
    "df_trip_fm = df_trip_fm.withColumn('flg_weekday',flgweekday(F.dayofweek(F.to_timestamp(col('pickup_datetime')))))\\\n",
    "    .withColumn('driver_revenue',col('fare_amount')+col('tip_amount'))\\\n",
    "        .withColumn('pickup_cat',F.concat_ws('_',col('pickup_period'),getabbrv(col('pickup_area'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### aggregat numeric \n",
    "\n",
    "df_daily_agg_numeric = df_trip_fm.groupBy('driver_id','pickup_date')\\\n",
    "    .agg(F.round(F.sum(col('trip_time_in_secs'))/3600.0,2).alias('hours'),\n",
    "    F.count(col('trip_id')).alias('trips'),\n",
    "    F.round(F.sum(col('driver_revenue')),2).alias('revenue'))\\\n",
    "        .withColumn('revenue_per_hour',F.round(col('revenue')/col('hours'),2))\\\n",
    "            .withColumn('minutes_per_trip',F.round(col('hours')*60.0/col('trips'),2))\n",
    "\n",
    "df_daily_agg_numeric.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### aggregate by trip\n",
    "\n",
    "simplify_name = udf(lambda x: str(x).lower().replace('.','_').replace('-<','lt').replace('-','to'),StringType())\n",
    "\n",
    "df_trip_pickup_cat = df_trip_fm.groupBy('driver_id','pickup_date','pickup_cat')\\\n",
    "    .agg(F.count(col('trip_id')).alias('trips')).\\\n",
    "        withColumn('pickup_cat',simplify_name(col('pickup_cat'))).toPandas()\n",
    "\n",
    "df_driver_date_tot_trip = df_trip_pickup_cat.groupby(by=['driver_id','pickup_date'],as_index=False)['trips'].sum()\n",
    "\n",
    "index_cols = ['driver_id','pickup_date']\n",
    "\n",
    "print(df_trip_pickup_cat.shape)\n",
    "df_trip_pickup_cat = df_trip_pickup_cat.merge(df_driver_date_tot_trip.rename(columns={'trips':'tot_trips'}),\n",
    "                                             on=index_cols,\n",
    "                                             how='left')\n",
    "print(df_trip_pickup_cat.shape)\n",
    "df_trip_pickup_cat['percentage'] = df_trip_pickup_cat.apply(lambda row: round(float(row['trips'])/row['tot_trips'],2),axis=1)\n",
    "\n",
    "df_trip_pickup_cat_tp = pd.pivot_table(df_trip_pickup_cat,\n",
    "                                        index=index_cols,\n",
    "                                        columns=['pickup_cat'],\n",
    "                                        values=['percentage']).fillna(0.0).reset_index()\n",
    "\n",
    "df_trip_pickup_cat_tp.columns = [v[0] if i<len(index_cols) else v[1] for i,v in enumerate(df_trip_pickup_cat_tp.columns.values)]\n",
    "\n",
    "df_trip_pickup_cat_spark = spark.createDataFrame(df_trip_pickup_cat_tp)\n",
    "df_trip_pickup_cat_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no. of taxi this driver occupied in a day\n",
    "\n",
    "df_taxi_per_day = spark.createDataFrame(df_taxi_driven_per_day_check.rename(columns={'count':'taxi_per_day'}))\n",
    "df_taxi_per_day.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join all features to predict Daily Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_keys = ['driver_id','pickup_date']\n",
    "print(df_daily_agg_numeric.count())\n",
    "df_driver_abt= df_daily_agg_numeric.join(df_trip_pickup_cat_spark, joining_keys, 'left')\n",
    "print(df_driver_abt.count())\n",
    "df_driver_abt= df_driver_abt.join(df_taxi_per_day, joining_keys, 'left')\n",
    "print(df_driver_abt.count())\n",
    "df_driver_abt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Avg daily revenue\n",
    "\n",
    "print(f\"avg daily revenue of Driver: {round(df_driver_abt_pd['revenue'].mean(),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to Pandas\n",
    "\n",
    "df_driver_abt_pd = df_driver_abt.toPandas()\n",
    "df_driver_abt_pd.shape\n",
    "\n",
    "### Set features\n",
    "non_feature_cols = ['driver_id','pickup_date','hours','revenue','revenue_per_hour']\n",
    "features_input = [i for i in df_driver_abt.columns if i not in non_feature_cols]\n",
    "features_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximising Daily Revenue for Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Test Split\n",
    "target = 'revenue'\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_driver_abt_pd[features_input], \n",
    "                                                    df_driver_abt_pd[target], \n",
    "                                                    random_state=9000, \n",
    "                                                    test_size=0.3)\n",
    "\n",
    "# Hyperparameters for GradientBoostingRegressor\n",
    "\n",
    "param_grid = {'n_estimators': [1000],\n",
    "          'max_depth': [3],\n",
    "          'min_samples_split': [5],\n",
    "          'learning_rate': [0.01],\n",
    "          'loss': ['ls']}\n",
    "\n",
    "# Create an instance of gradient boosting regressor\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "gbr_grid = GridSearchCV(estimator = gbr, param_grid = param_grid, \n",
    "                          cv = 2, n_jobs = -1, verbose = 1)\n",
    "# Fit the model\n",
    "\n",
    "gbr_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print Coefficient of determination R^2\n",
    "\n",
    "print(\"Model Accuracy: {:.3f}\".format(gbr_grid.score(X_test, y_test)))\n",
    "\n",
    "# Create the mean squared error\n",
    "\n",
    "mse = mean_squared_error(y_test, gbr_grid.predict(X_test), squared=False)\n",
    "print(\"Root mean squared error (RMSE) on test set: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature importance data using feature_importances_ attribute\n",
    "\n",
    "feature_importance = gbr_grid.best_estimator_.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(X_test.columns.values)[sorted_idx])\n",
    "plt.title('Daily Revenue Maximization: Feature Importance')\n",
    "result = permutation_importance(gbr_grid, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5, figsize=(15,5))\n",
    "for ax,feat in zip(axes,np.array(X_test.columns.values)[sorted_idx][-5:]):\n",
    "    print(ax)\n",
    "    print(feat)\n",
    "    plot_partial_dependence(gbr_grid,X_test,\n",
    "    [feat],\n",
    "    subsample=5,\n",
    "    n_jobs=-1,\n",
    "    grid_resolution=20,\n",
    "    ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save model\n",
    "gbr_grid.save('models/driver_daily_revenue_predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing revenue_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove outliers who work 0.0 hours\n",
    "df_driver_abt_rph = df_driver_abt_pd.iloc[np.where(df_driver_abt_pd['revenue_per_hour'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### impute extreme revenue per hour\n",
    "\n",
    "rph_cutoff = np.percentile(df_driver_abt_rph['revenue_per_hour'],[0,1,5,25,50,75,95,99,100])[-2]\n",
    "\n",
    "df_driver_abt_rph['revenue_per_hour'] = np.where(df_driver_abt_rph['revenue_per_hour']>rph_cutoff,\n",
    "                                                rph_cutoff,\n",
    "                                                df_driver_abt_rph['revenue_per_hour'])\n",
    "\n",
    "### check daily hours\n",
    "\n",
    "np.percentile(df_driver_abt_rph['hours'],[0,1,5,25,50,75,95,99,100])\n",
    "\n",
    "### drop those who work less than 1 hour\n",
    "print(df_driver_abt_rph.shape)\n",
    "df_driver_abt_rph_fltrd = df_driver_abt_rph.iloc[np.where(df_driver_abt_rph['hours']>=1)].reset_index(drop=True)\n",
    "print(df_driver_abt_rph_fltrd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set features\n",
    "non_feature_cols = ['driver_id','pickup_date','revenue','revenue_per_hour','trips','minutes_per_trip']\n",
    "features_input = [i for i in df_driver_abt_rph_fltrd.columns if i not in non_feature_cols]\n",
    "\n",
    "# Create Training and Test Split\n",
    "target = 'revenue_per_hour'\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_driver_abt_rph_fltrd[features_input], \n",
    "                                                    df_driver_abt_rph_fltrd[target], \n",
    "                                                    random_state=9000, \n",
    "                                                    test_size=0.3)\n",
    "\n",
    "# Hyperparameters for GradientBoostingRegressor\n",
    "\n",
    "param_grid = {'n_estimators': [1000],\n",
    "          'max_depth': [3],\n",
    "          'min_samples_split': [5],\n",
    "          'learning_rate': [0.01],\n",
    "          'loss': ['ls']}\n",
    "\n",
    "# Create an instance of gradient boosting regressor\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "gbr_grid_rph = GridSearchCV(estimator = gbr, param_grid = param_grid, \n",
    "                          cv = 2, n_jobs = -1, verbose = 1)\n",
    "# Fit the model\n",
    "\n",
    "gbr_grid_rph.fit(X_train, y_train)\n",
    "\n",
    "# Print Coefficient of determination R^2\n",
    "\n",
    "print(\"Model Accuracy: {:.3f}\".format(gbr_grid_rph.score(X_test, y_test)))\n",
    "\n",
    "# Create the mean squared error\n",
    "\n",
    "mse = mean_squared_error(y_test, gbr_grid_rph.predict(X_test), squared=False)\n",
    "print(\"Root mean squared error (RMSE) on test set: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature importance data using feature_importances_ attribute\n",
    "\n",
    "feature_importance = gbr_grid_rph.best_estimator_.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(X_test.columns.values)[sorted_idx])\n",
    "plt.title('Revenue Per Hour Maximization: Feature Importance')\n",
    "result = permutation_importance(gbr_grid_rph, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5, figsize=(15,5))\n",
    "for ax,feat in zip(axes,np.array(X_test.columns.values)[sorted_idx][-5:]):\n",
    "    print(ax)\n",
    "    print(feat)\n",
    "    plot_partial_dependence(gbr_grid_rph,X_test,\n",
    "    [feat],\n",
    "    subsample=5,\n",
    "    n_jobs=-1,\n",
    "    grid_resolution=20,\n",
    "    ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save model\n",
    "gbr_grid_rph.save('models/driver_daily_rev_per_hour_predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi - Daily level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### aggregat numeric \n",
    "\n",
    "df_taxi_daily_agg_numeric = df_trip_fm.groupBy('taxi_id','pickup_date')\\\n",
    "    .agg(F.round(F.sum(col('trip_time_in_secs'))/3600.0,2).alias('hours'),\n",
    "    F.count(col('trip_id')).alias('trips'),\n",
    "    F.round(F.sum(col('driver_revenue')),2).alias('revenue'))\\\n",
    "        .withColumn('revenue_per_hour',F.round(col('revenue')/col('hours'),2))\\\n",
    "            .withColumn('minutes_per_trip',F.round(col('hours')*60.0/col('trips'),2))\n",
    "\n",
    "df_taxi_daily_agg_numeric.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### number of drivers ocuupying taxi per day\n",
    "\n",
    "df_driver_occ_taxi_per_day_check = df_trip_fm.groupBy('taxi_id','pickup_date','driver_id').count()\\\n",
    "    .groupBy('taxi_id','pickup_date').count().sort(F.desc('count')).toPandas()\n",
    "\n",
    "# df_driver_occ_taxi_per_day_check.groupby(by='count',as_index=False)['taxi_id'].count().sort_values(by='count',ascending=0)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# Remove suspicious data of taxi_id - pickup_date combo occupied by more than 3 drivers in a day\n",
    "drivers_per_day_limit = 3\n",
    "df_combo_taxi_to_drop = df_driver_occ_taxi_per_day_check.iloc[np.where(df_driver_occ_taxi_per_day_check['count']>drivers_per_day_limit)].drop(columns='count',axis=1)\n",
    "\n",
    "#drivers occupying taxi in a day\ttaxi-days\n",
    "#\t24\t1\n",
    "#\t13\t1\n",
    "#\t5\t2\n",
    "#\t4\t484\n",
    "#\t3\t53320\n",
    "#\t2\t248957\n",
    "#\t1\t81583\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_driver_occ_taxi_per_day_check.groupby(by='count',as_index=False)['taxi_id'].count().rename(columns={'taxi_id':'no of taxi-day','count':'drivers occupying in a day'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_trip_fm.count())\n",
    "df_combo_taxi_to_drop_spark =spark.createDataFrame(df_combo_taxi_to_drop) \n",
    "\n",
    "joining_conds = [df_trip_fm.taxi_id == df_combo_taxi_to_drop_spark.taxi_id,\n",
    "                 df_trip_fm.pickup_date == df_combo_taxi_to_drop_spark.pickup_date]\n",
    "df_trip_fm = df_trip_fm.join(df_combo_taxi_to_drop_spark, joining_conds, 'leftanti')\n",
    "print(df_trip_fm.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no. of drivers taxi was occupied by in a day\n",
    "\n",
    "df_driver_per_day = spark.createDataFrame(df_driver_occ_taxi_per_day_check.rename(columns={'count':'driver_per_day'}))\n",
    "df_driver_per_day.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_keys = ['taxi_id','pickup_date']\n",
    "print(df_taxi_daily_agg_numeric.count())\n",
    "df_taxi_abt= df_taxi_daily_agg_numeric.join(df_driver_per_day, joining_keys, 'left')\n",
    "print(df_taxi_abt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check correlation between revenue, hours and no. of drivers occupying taxi per day\n",
    "vector_col = \"corr_features\"\n",
    "cols_to_check_corr = ['revenue','hours','driver_per_day']\n",
    "df = df_taxi_abt.select(*[cols_to_check_corr]).toPandas()\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find example case of taxi occupied by multiple drivers in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_driver_occ_taxi_per_day_check.iloc[np.where(df_driver_occ_taxi_per_day_check['count']==3)]\n",
    "\n",
    "# taxi_id\tpickup_date\tcount\n",
    "# 8892\t2013-04-28\t3\n",
    "# 11086\t2013-04-06\t3\n",
    "\n",
    "df_trip_fm.filter((col('taxi_id')==8892)&(col('pickup_date')=='2013-04-28')).sort(col('pickup_datetime').asc())\\\n",
    "    .select('taxi_id','driver_id','pickup_datetime','trip_time_in_secs').show(50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c818777a6ad54da25b094a50dfb6e8ddd7f4bf040e5d0005aff88d6d371533f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
